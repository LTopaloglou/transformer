{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5J/f7OKBepLpdvf8S/FaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LTopaloglou/transformer/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8rux9qiuqpLV"
      },
      "outputs": [],
      "source": [
        "from torch.torch_version import Version\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#I am using variable names defined in \"Attention Is All You Need\" for simplicity.\n",
        "\n",
        "#Define a single Scaled Dot-Product Attention head. I am simplifying it by enforcing d_k = d_v\n",
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_k, mask):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_k = d_k\n",
        "    self.register_buffer('lowerTriangle', torch.tril(torch.ones(d_k, d_k)))\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #Takes in vectors of Queries, Keys & Values. Queries, Keys & Values have dimension seq x d_k\n",
        "    if (Q.shape[1] != self.d_k or K.shape[1] != self.d_k or V.shape[1] != self.d_k):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    #First take the dot product of Queries & Keys. Weight has dimensions seq x seq\n",
        "    weight = Q @ K.transpose(0, 1)\n",
        "    #Now scale by 1/sqrt(d_k)\n",
        "    weight = weight  * (self.d_k**-0.5)\n",
        "    #Mask everything not in the lower triangular of weight\n",
        "    if (self.mask):\n",
        "      weight = weight.masked_fill(self.lowerTriangle== 0, float('-inf'))\n",
        "    #Now apply softmax in the dimension of the rows\n",
        "    weight = weight.softmax(1)\n",
        "    #Finally, multiply values by weights to get the outputs\n",
        "    out = weight @ V # [seq, seq] x [seq, d_k] = [seq, d_k]\n",
        "    return out\n",
        "\n",
        "#Define a Multi-Head Attention layer. As in the paper, I am setting d_k = d_model/h\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, h, mask):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    self.d_k = int(d_model/h)\n",
        "    if (self.d_k != d_model/h):\n",
        "      raise Exception('Invalid Dimensions Provided') #Ensure valid dimensions\n",
        "    self.W_O = nn.Linear(h*self.d_k, d_model, bias=False) #Output linear layer\n",
        "    self.W_Q = nn.ModuleList() #The h different Query linear layers\n",
        "    self.W_K = nn.ModuleList() #The h different Key linear layers\n",
        "    self.W_V = nn.ModuleList() #The h different Value linear layers\n",
        "    self.Att = Attention(self.d_k, mask) #I think (?) only 1 attention layer is needed since no backprop happens TODO: VERIFY\n",
        "    for i in range(h):\n",
        "      #Initialize all h linear layers for Q, K, V\n",
        "      self.W_Q.append(nn.Linear(d_model, self.d_k, bias=False))\n",
        "      self.W_K.append(nn.Linear(d_model, self.d_k, bias=False))\n",
        "      self.W_V.append(nn.Linear(d_model, self.d_k, bias=False)) #TODO: Instead of Linear layers, just create Matrices\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #The inputs are Queries, Keys and Vectors which each have size seq x d_model\n",
        "    if (Q.shape[1] != self.d_model or K.shape[1] != self.d_model or V.shape[1] != self.d_model):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    heads = []\n",
        "    for i in range(self.h):\n",
        "      queries = self.W_Q[i](Q)\n",
        "      keys = self.W_K[i](K)\n",
        "      values = self.W_V[i](V)\n",
        "      #At this point, queries keys & values have dimensions seq x d_k\n",
        "      heads.append(self.Att.forward(queries, keys, values))\n",
        "    out = torch.cat(heads, 1) #The output has the same dimension as all the inputs: seq x d_model\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of Attention\n",
        "att = Attention(5, True)\n",
        "keys = torch.ones(5, 1)\n",
        "queries = torch.ones(5, 1)\n",
        "values = torch.ones(5, 1)\n",
        "print(att.forward(queries, keys, values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOooydTg0Xw9",
        "outputId": "0a7e7a32-7a13-44e0-aff8-b22e4edf46ef"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of MultiHeadAttention\n",
        "#Input is sequence size by d_model\n",
        "#Q, K, V are all that size - in self attention Q=K=V=Input\n",
        "#Hyperparams\n",
        "seq = 10 #Sequence size of 4\n",
        "d_model = 16 #Embedding vectors are of size 16\n",
        "heads = 8 #Number of heads in our multihead attention\n",
        "mult = MultiHeadAttention(d_model, heads, False)\n",
        "Input = torch.ones(seq, d_model)\n",
        "mult.forward(Input, Input, Input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7vyhFYWCYUz",
        "outputId": "7d973a4d-9546-475e-889b-81d228783de8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 16])\n",
            "tensor([[ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706],\n",
            "        [ 0.4809,  0.6958,  0.0646,  0.2008, -0.7846, -0.6914, -0.3471,  0.0936,\n",
            "          0.4514, -0.0206, -0.0639,  0.2451,  0.0504, -0.9306, -0.2231,  0.5706]],\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    }
  ]
}