{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEg4QgzEwlqGcK2yu/rMgV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LTopaloglou/transformer/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "8rux9qiuqpLV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#Setup hyperparameters.\n",
        "\n",
        "\n",
        "#Define a single Scaled Dot-Product Attention head\n",
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_k, mask):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_k = d_k\n",
        "    self.register_buffer('lowerTriangle', torch.tril(torch.ones(d_k, d_k)))\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #Takes in vectors of Queries, Keys & Values. Queries, Keys & Values have dimension d_k x 1 (one dimensional vectors)\n",
        "    if (Q.shape[0] != self.d_k or K.shape[0] != self.d_k or V.shape[0] != self.d_k):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    #First take the dot product of Queries & Keys\n",
        "    weight = Q @ K.transpose(0, 1)\n",
        "    #Now scale by 1/sqrt(d_k)\n",
        "    weight = weight  * (self.d_k**-0.5)\n",
        "    #Mask everything not in the lower triangular of weight\n",
        "    if (self.mask):\n",
        "      weight = weight.masked_fill(self.lowerTriangle== 0, float('-inf'))\n",
        "    #Now apply softmax in the dimension of the rows\n",
        "    weight = weight.softmax(1)\n",
        "    #Finally, multiply values by weights to get the outputs\n",
        "    out = weight @ V # [d_k, d_k] x [d_k, 1] = [d_k, 1]\n",
        "    return out\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation\n",
        "att = Attention(5, True)\n",
        "keys = torch.ones(5, 1)\n",
        "queries = torch.ones(5, 1)\n",
        "values = torch.ones(5, 1)\n",
        "print(att.forward(queries, keys, values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOooydTg0Xw9",
        "outputId": "0a7e7a32-7a13-44e0-aff8-b22e4edf46ef"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    }
  ]
}