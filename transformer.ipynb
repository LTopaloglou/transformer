{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOTBUl/nevgPu6e8uS9SgN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LTopaloglou/transformer/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rux9qiuqpLV"
      },
      "outputs": [],
      "source": [
        "from torch.torch_version import Version\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#I am using variable names defined in \"Attention Is All You Need\" for simplicity.\n",
        "\n",
        "#Global Hyperparams\n",
        "block_size = 8 #The max number of characters of context used. seq <= block_size always\n",
        "\n",
        "#Define a single Scaled Dot-Product Attention head. I am simplifying it by enforcing d_k = d_v\n",
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_k, mask):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_k = d_k\n",
        "    self.register_buffer('lowerTriangle', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #Takes in vectors of Queries, Keys & Values. Queries, Keys & Values have dimension seq x d_k\n",
        "    if (Q.shape[1] != self.d_k or K.shape[1] != self.d_k or V.shape[1] != self.d_k):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    seq = Q.shape[0]\n",
        "    #First take the dot product of Queries & Keys. Weight has dimensions seq x seq\n",
        "    weight = Q @ K.transpose(0, 1)\n",
        "    #Now scale by 1/sqrt(d_k)\n",
        "    weight = weight  * (self.d_k**-0.5)\n",
        "    #Mask everything not in the lower triangular of weight\n",
        "    if (self.mask):\n",
        "      weight = weight.masked_fill(self.lowerTriangle[:seq, :seq]== 0, float('-inf'))\n",
        "    #Now apply softmax in the dimension of the rows\n",
        "    weight = weight.softmax(1)\n",
        "    #Finally, multiply values by weights to get the outputs\n",
        "    out = weight @ V # [seq, seq] x [seq, d_k] = [seq, d_k]\n",
        "    return out\n",
        "\n",
        "#Define a Multi-Head Attention layer. As in the paper, I am setting d_k = d_model/h\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, h, mask):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    self.d_k = int(d_model/h)\n",
        "    if (self.d_k != d_model/h):\n",
        "      raise Exception('Invalid Dimensions Provided') #Ensure valid dimensions\n",
        "    self.W_O = nn.Linear(h*self.d_k, d_model, bias=False) #Output linear layer\n",
        "    self.W_Q = nn.ModuleList() #The h different Query linear layers\n",
        "    self.W_K = nn.ModuleList() #The h different Key linear layers\n",
        "    self.W_V = nn.ModuleList() #The h different Value linear layers\n",
        "    self.Att = Attention(self.d_k, mask) #I think (?) only 1 attention layer is needed since no backprop happens TODO: VERIFY\n",
        "    for i in range(h):\n",
        "      #Initialize all h linear layers for Q, K, V\n",
        "      self.W_Q.append(nn.Linear(d_model, self.d_k, bias=False))\n",
        "      self.W_K.append(nn.Linear(d_model, self.d_k, bias=False))\n",
        "      self.W_V.append(nn.Linear(d_model, self.d_k, bias=False)) #TODO: Instead of Linear layers, just create Matrices\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #The inputs are Queries, Keys and Vectors which each have size seq x d_model\n",
        "    if (Q.shape[1] != self.d_model or K.shape[1] != self.d_model or V.shape[1] != self.d_model):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    heads = []\n",
        "    for i in range(self.h):\n",
        "      queries = self.W_Q[i](Q)\n",
        "      keys = self.W_K[i](K)\n",
        "      values = self.W_V[i](V)\n",
        "      #At this point, queries keys & values have dimensions seq x d_k\n",
        "      heads.append(self.Att.forward(queries, keys, values))\n",
        "    out = torch.cat(heads, 1) #The output has the same dimension as all the inputs: seq x d_model\n",
        "    out = self.W_O(out)\n",
        "    return out\n",
        "\n",
        "#Define a Feed Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_hidden):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(d_model, d_hidden, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_hidden, d_model, bias=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.network(x)\n",
        "    return x\n",
        "\n",
        "#Define Layer Normalization:\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.epsilon = 1e-5\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "  def __call__(self, x):\n",
        "    mean = x.mean(1, keepdim=True) #Mean across the layer i.e. the column\n",
        "    variance = x.var(1, keepdim=True) #Mean across the layer i.e. column\n",
        "    norm = (x - mean) / torch.sqrt(variance + self.epsilon) #Normalize\n",
        "    out = self.gamma * norm + self.beta #Scale by gamma, add beta to achieve var= gamma, mean = beta\n",
        "    return out\n",
        "\n",
        "#Define an Block of Multi Head Self-Attention with a Residual Connection & Layer Normalization\n",
        "class NormalizedSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, h, mask):\n",
        "    super().__init__()\n",
        "    self.MHA = MultiHeadAttention(d_model, h, mask)\n",
        "    self.norm = LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.MHA(x, x, x)\n",
        "    x = self.norm(x)\n",
        "    return x\n",
        "\n",
        "#Define a block of a Feed Forward Network with a Residual Connection & Layer Normalization\n",
        "class NormalizedFeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_hidden):\n",
        "    super().__init__()\n",
        "    self.FF = FeedForward(d_model, d_hidden)\n",
        "    self.norm = LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.FF(x)\n",
        "    x = self.norm(x)\n",
        "    return x\n",
        "\n",
        "#Define a stand-alone Decoder block with N self-attention and feed forward blocks (without embeddings or softmax)\n",
        "#Note that the self attention is Masked because this is a Decoder\n",
        "class StandAloneDecoder(nn.Module):\n",
        "\n",
        "  def __init__(self, N, d_model, h, d_hidden):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential()\n",
        "    for i in range(N):\n",
        "      self.network.append(NormalizedSelfAttention(d_model, h, True))\n",
        "      self.network.append(NormalizedFeedForward(d_model, d_hidden))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.network(x)\n",
        "    return out\n",
        "\n",
        "#Define an entire Transformer with E encoder blocks and D decoder blocks (without embeddings or softmax)\n",
        "#The last encoder provides input to all D decoders\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, E, D, d_model, h, d_hidden):\n",
        "    super().__init__()\n",
        "    self.E = E\n",
        "    self.D = D\n",
        "    self.encoders = nn.Sequential()\n",
        "    for e in range(E):\n",
        "      self.encoders.append(NormalizedSelfAttention(d_model, h, False))\n",
        "      self.encoders.append(NormalizedFeedForward(d_model, d_hidden))\n",
        "    self.decoders = nn.ModuleList()\n",
        "    for d in range(D):\n",
        "      self.decoders.append(NormalizedSelfAttention(d_model, h, True))\n",
        "      self.decoders.append(MultiHeadAttention(d_model, h, False))\n",
        "      self.decoders.append(LayerNorm(d_model))\n",
        "      self.decoders.append(NormalizedFeedForward(d_model, d_hidden))\n",
        "\n",
        "  def __forward__(self, inputs, outputs):\n",
        "    inputs = self.encoders(inputs)\n",
        "    for e in range(self.E):\n",
        "      outputs = self.decoders[3*e](outputs) #self attention\n",
        "      outputs = self.decoders[3*e+1](outputs, inputs, inputs) #cross attention\n",
        "      outputs = outputs + self.decoders[3*e+2](outputs) #Layer norm & residual connection\n",
        "      outputs = self.decoders[3*e+3](outputs) #Feed forward\n",
        "    return outputs\n",
        "#Test what happens when a cross attention head gets inputs of:\n",
        "#Queries: a x d_model\n",
        "#Keys & Values: b x d_model\n",
        "#Where a != b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing NormalizedFeedForward\n",
        "d_model = 32\n",
        "d_hidd = 64\n",
        "seq = 3\n",
        "normFeed = NormalizedFeedForward(d_model, d_hidd)\n",
        "input = torch.ones(seq, d_model)\n",
        "output = normFeed(input)\n",
        "print(output)\n",
        "print(output[0,:].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwxGsN8KtNno",
        "outputId": "23e14db1-f96f-4fd1-d83d-56ccbf72cfd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.8433, -0.3277,  0.8559,  0.8177,  0.6663, -0.3877, -0.4108,  2.0639,\n",
            "         -1.2623,  1.3789, -0.2999, -0.0571,  0.1014, -0.0150, -0.4671, -0.1986,\n",
            "         -0.6606,  2.4152, -0.7222, -1.7776,  0.9840,  0.7970, -0.5530, -0.9412,\n",
            "         -1.7836, -1.0741, -0.3088, -0.9661,  0.3586, -0.5448,  1.1079,  0.3682],\n",
            "        [ 0.8433, -0.3277,  0.8559,  0.8177,  0.6663, -0.3877, -0.4108,  2.0639,\n",
            "         -1.2623,  1.3789, -0.2999, -0.0571,  0.1014, -0.0150, -0.4671, -0.1986,\n",
            "         -0.6606,  2.4152, -0.7222, -1.7776,  0.9840,  0.7970, -0.5530, -0.9412,\n",
            "         -1.7836, -1.0741, -0.3088, -0.9661,  0.3586, -0.5448,  1.1079,  0.3682],\n",
            "        [ 0.8433, -0.3277,  0.8559,  0.8177,  0.6663, -0.3877, -0.4108,  2.0639,\n",
            "         -1.2623,  1.3789, -0.2999, -0.0571,  0.1014, -0.0150, -0.4671, -0.1986,\n",
            "         -0.6606,  2.4152, -0.7222, -1.7776,  0.9840,  0.7970, -0.5530, -0.9412,\n",
            "         -1.7836, -1.0741, -0.3088, -0.9661,  0.3586, -0.5448,  1.1079,  0.3682]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor(0.9999, grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing NormalizedSelfAttention\n",
        "d_model = 32\n",
        "heads = 8\n",
        "seq = 3\n",
        "selfAtt = NormalizedSelfAttention(d_model, heads, True)\n",
        "input = torch.ones(seq, d_model)\n",
        "output = selfAtt(input)\n",
        "print(output)\n",
        "print(output[0,:].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0sX2BGFsdst",
        "outputId": "3945d989-517e-4159-eacd-16395893b5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2899, -0.0913, -0.1104,  0.7696,  0.0718,  0.2509,  0.3303, -1.9164,\n",
            "         -0.0496,  0.5569,  0.1692, -0.1273, -1.1905,  0.4290, -0.8969, -0.1584,\n",
            "         -0.1496, -1.1824, -0.4902,  0.4242, -0.4168, -0.4957,  0.9929, -2.2237,\n",
            "         -0.8495, -0.5435,  1.3629, -0.7239,  2.6632,  1.3310,  1.2465, -0.2724],\n",
            "        [ 1.2899, -0.0913, -0.1104,  0.7696,  0.0718,  0.2509,  0.3303, -1.9164,\n",
            "         -0.0496,  0.5569,  0.1692, -0.1273, -1.1905,  0.4290, -0.8969, -0.1584,\n",
            "         -0.1496, -1.1824, -0.4902,  0.4242, -0.4168, -0.4957,  0.9929, -2.2237,\n",
            "         -0.8495, -0.5435,  1.3629, -0.7239,  2.6632,  1.3310,  1.2465, -0.2724],\n",
            "        [ 1.2899, -0.0913, -0.1104,  0.7696,  0.0718,  0.2509,  0.3303, -1.9164,\n",
            "         -0.0496,  0.5569,  0.1692, -0.1273, -1.1905,  0.4290, -0.8969, -0.1584,\n",
            "         -0.1496, -1.1824, -0.4902,  0.4242, -0.4168, -0.4957,  0.9929, -2.2237,\n",
            "         -0.8495, -0.5435,  1.3629, -0.7239,  2.6632,  1.3310,  1.2465, -0.2724]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor(0.9999, grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing layer norm\n",
        "randoms = torch.randn(2, 5)\n",
        "layernorm = LayerNorm(5)\n",
        "randoms = layernorm(randoms)\n",
        "\n",
        "print(\"Column mean and std dev\")\n",
        "print(randoms[:,0].mean())\n",
        "print(randoms[:,0].std())\n",
        "\n",
        "print(\"Row mean and std dev\")\n",
        "print(randoms[0,:].mean())\n",
        "print(randoms[0,:].std())\n",
        "\n",
        "print(\"We want the rows normalized\")\n",
        "print(randoms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl2QD0Muit60",
        "outputId": "9c9c5e7c-46c6-4fa1-b8ec-a5c01dc3b0ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column mean and std dev\n",
            "tensor(0.8183)\n",
            "tensor(0.1750)\n",
            "Row mean and std dev\n",
            "tensor(-1.1921e-08)\n",
            "tensor(1.0000)\n",
            "We want the rows normalized\n",
            "tensor([[ 0.9421, -1.5342,  0.7111,  0.2911, -0.4101],\n",
            "        [ 0.6945,  0.5012,  0.8204, -1.5376, -0.4785]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of Feed Forward\n",
        "d_model = 64\n",
        "d_hidden = 256\n",
        "seq = 5\n",
        "ff = FeedForward(d_model, d_hidden)\n",
        "Input = torch.ones(seq, d_model)\n",
        "print(ff.forward(Input).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pj7CDHlR-22",
        "outputId": "724e0462-d128-4691-f083-1be5eb6bc2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of MultiHeadAttention\n",
        "#Input is sequence size by d_model\n",
        "#Q, K, V are all that size - in self attention Q=K=V=Input\n",
        "#Hyperparams:\n",
        "seq = 8 #Sequence size (i.e. number of characters). Must be less than blocksize.\n",
        "d_model = 16 #Size of the embedding vectors for each character\n",
        "heads = 8 #Number of heads in our multihead attention\n",
        "mult = MultiHeadAttention(d_model, heads, True)\n",
        "Input = torch.ones(seq, d_model)\n",
        "output = mult.forward(Input, Input, Input)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7vyhFYWCYUz",
        "outputId": "78985d9b-e7d8-434b-decf-3fb2fc1fcaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 16])\n",
            "tensor([[-0.6204, -0.5655, -0.3487,  0.4491, -0.6671,  0.5785, -0.7576,  0.1988,\n",
            "          0.4697,  0.6004, -0.5643, -0.0136,  0.8412, -0.1806,  0.3783,  0.0433],\n",
            "        [-0.6204, -0.5655, -0.3487,  0.4491, -0.6671,  0.5785, -0.7576,  0.1988,\n",
            "          0.4697,  0.6004, -0.5643, -0.0136,  0.8412, -0.1806,  0.3783,  0.0433],\n",
            "        [-0.6204, -0.5655, -0.3487,  0.4491, -0.6671,  0.5785, -0.7576,  0.1988,\n",
            "          0.4697,  0.6004, -0.5643, -0.0136,  0.8412, -0.1806,  0.3783,  0.0433],\n",
            "        [-0.6204, -0.5655, -0.3487,  0.4491, -0.6671,  0.5785, -0.7576,  0.1988,\n",
            "          0.4697,  0.6004, -0.5643, -0.0136,  0.8412, -0.1806,  0.3783,  0.0433],\n",
            "        [-0.6204, -0.5655, -0.3487,  0.4491, -0.6671,  0.5785, -0.7576,  0.1988,\n",
            "          0.4697,  0.6004, -0.5643, -0.0136,  0.8412, -0.1806,  0.3783,  0.0433],\n",
            "        [-0.6204, -0.5655, -0.3487,  0.4491, -0.6671,  0.5785, -0.7576,  0.1988,\n",
            "          0.4697,  0.6004, -0.5643, -0.0136,  0.8412, -0.1806,  0.3783,  0.0433],\n",
            "        [-0.6204, -0.5655, -0.3487,  0.4491, -0.6671,  0.5785, -0.7576,  0.1988,\n",
            "          0.4697,  0.6004, -0.5643, -0.0136,  0.8412, -0.1806,  0.3783,  0.0433],\n",
            "        [-0.6204, -0.5655, -0.3487,  0.4491, -0.6671,  0.5785, -0.7576,  0.1988,\n",
            "          0.4697,  0.6004, -0.5643, -0.0136,  0.8412, -0.1806,  0.3783,  0.0433]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of Attention\n",
        "#Q, K, V all have size seq x d_k\n",
        "#Hyperparams:\n",
        "seq = 4 #Sequence length (i.e. number of characters)\n",
        "d_k = 5 #dimension of embedding vectors in this head\n",
        "att = Attention(d_k, True)\n",
        "Input = torch.ones(seq, d_k)\n",
        "print(att.forward(Input, Input, Input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOooydTg0Xw9",
        "outputId": "acebd3e0-0be7-42dc-f4ec-64fb1a161a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    }
  ]
}