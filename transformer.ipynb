{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLM0NWMFZ9VbQ1jr6SPZtN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LTopaloglou/transformer/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "8rux9qiuqpLV"
      },
      "outputs": [],
      "source": [
        "from torch.torch_version import Version\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#I am using variable names defined in \"Attention Is All You Need\" for simplicity.\n",
        "\n",
        "#Define a single Scaled Dot-Product Attention head. I am simplifying it by enforcing d_k = d_v\n",
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_k, mask):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_k = d_k\n",
        "    self.register_buffer('lowerTriangle', torch.tril(torch.ones(d_k, d_k)))\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #Takes in vectors of Queries, Keys & Values. Queries, Keys & Values have dimension d_k x 1 (one dimensional vectors)\n",
        "    if (Q.shape[0] != self.d_k or K.shape[0] != self.d_k or V.shape[0] != self.d_k):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    #First take the dot product of Queries & Keys\n",
        "    weight = Q @ K.transpose(0, 1)\n",
        "    #Now scale by 1/sqrt(d_k)\n",
        "    weight = weight  * (self.d_k**-0.5)\n",
        "    #Mask everything not in the lower triangular of weight\n",
        "    if (self.mask):\n",
        "      weight = weight.masked_fill(self.lowerTriangle== 0, float('-inf'))\n",
        "    #Now apply softmax in the dimension of the rows\n",
        "    weight = weight.softmax(1)\n",
        "    #Finally, multiply values by weights to get the outputs\n",
        "    out = weight @ V # [d_k, d_k] x [d_k, 1] = [d_k, 1]\n",
        "    return out\n",
        "\n",
        "#Define a Multi-Head Attention layer. As in the paper, I am setting d_k = d_model/h\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, h, mask):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    self.d_k = int(d_model/h)\n",
        "    if (self.d_k != d_model/h):\n",
        "      raise Exception('Invalid Dimensions Provided') #Ensure valid dimensions\n",
        "    self.W_O = nn.Linear(h*self.d_k, d_model, bias=False) #Output linear layer\n",
        "    self.W_Q = nn.ModuleList() #The h different Query linear layers\n",
        "    self.W_K = nn.ModuleList() #The h different Key linear layers\n",
        "    self.W_V = nn.ModuleList() #The h different Value linear layers\n",
        "    self.Att = Attention(self.d_k, mask) #I think (?) only 1 attention layer is needed since no backprop happens TODO: VERIFY\n",
        "    for i in range(h):\n",
        "      #Initialize all h linear layers for Q, K, V\n",
        "      self.W_Q.append(nn.Linear(d_model, self.d_k, bias=False))\n",
        "      self.W_K.append(nn.Linear(d_model, self.d_k, bias=False))\n",
        "      self.W_V.append(nn.Linear(d_model, self.d_k, bias=False)) #TODO: Instead of Linear layers, just create Matrices\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #The inputs are Queries, Keys and Vectors which each have size d_model x 1\n",
        "    if (Q.shape[0] != self.d_model or K.shape[0] != self.d_model or V.shape[0] != self.d_model):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    heads = []\n",
        "    for i in range(self.h):\n",
        "      queries = self.W_Q[i].weight(Q)\n",
        "      keys = self.W_K[i].weight(K)\n",
        "      values = self.W_V[i].weight(V)\n",
        "      heads.append(self.Att.forward(Q, K, V))\n",
        "    out = torch.cat(heads, 0)\n",
        "    print(out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of Attention\n",
        "att = Attention(5, True)\n",
        "keys = torch.ones(5, 1)\n",
        "queries = torch.ones(5, 1)\n",
        "values = torch.ones(5, 1)\n",
        "print(att.forward(queries, keys, values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOooydTg0Xw9",
        "outputId": "0a7e7a32-7a13-44e0-aff8-b22e4edf46ef"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of MultiHeadAttention\n",
        "mult = MultiHeadAttention(16, 4, False)\n",
        "Q = torch.ones(16, 1)\n",
        "K = torch.ones(16, 1)\n",
        "V = torch.ones(16, 1)\n",
        "#TODO: What is the dimensions of Q, K, V in the paper? Must be a matrix?\n",
        "print(Q)\n",
        "mult.forward(Q, K, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "A7vyhFYWCYUz",
        "outputId": "9f6bc5a1-2f7c-42f3-9cce-c6c5623fd83f"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-24c1e13094bd>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-101-6ce8adf2f127>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Q, K, V)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mheads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m       \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_Q\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m       \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_K\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_V\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Parameter' object is not callable"
          ]
        }
      ]
    }
  ]
}