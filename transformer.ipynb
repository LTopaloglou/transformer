{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LTopaloglou/transformer/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rux9qiuqpLV"
      },
      "outputs": [],
      "source": [
        "from torch.torch_version import Version\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#I am using variable names defined in \"Attention Is All You Need\" for simplicity.\n",
        "\n",
        "#Define a single Scaled Dot-Product Attention head. I am simplifying it by enforcing d_k = d_v\n",
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_k, mask, context_len):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_k = d_k\n",
        "    self.register_buffer('lowerTriangle', torch.tril(torch.ones(context_len, context_len)))\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #Takes in vectors of Queries, Keys & Values. Queries, Keys & Values have dimension seq x d_k\n",
        "    if (Q.shape[1] != self.d_k or K.shape[1] != self.d_k or V.shape[1] != self.d_k):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    seq = Q.shape[0]\n",
        "    #First take the dot product of Queries & Keys. Weight has dimensions seq x seq\n",
        "    weight = Q @ K.transpose(0, 1)\n",
        "    #Now scale by 1/sqrt(d_k)\n",
        "    weight = weight  * (self.d_k**-0.5)\n",
        "    #Mask everything not in the lower triangular of weight\n",
        "    if (self.mask):\n",
        "      weight = weight.masked_fill(self.lowerTriangle[:seq, :seq]== 0, float('-inf'))\n",
        "    #Now apply softmax in the dimension of the rows\n",
        "    weight = weight.softmax(1)\n",
        "    #Finally, multiply values by weights to get the outputs\n",
        "    out = weight @ V # [seq, seq] x [seq, d_k] = [seq, d_k]\n",
        "    return out\n",
        "\n",
        "#Define a Multi-Head Attention layer. As in the paper, I am setting d_k = d_model/h\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, h, mask, context_len):\n",
        "    super().__init__()\n",
        "    self.mask = mask\n",
        "    self.d_model = d_model\n",
        "    self.h = h\n",
        "    self.d_k = int(d_model/h)\n",
        "    if (self.d_k != d_model/h):\n",
        "      raise Exception('Invalid Dimensions Provided') #Ensure valid dimensions\n",
        "    self.W_O = nn.Linear(h*self.d_k, d_model, bias=False) #Output linear layer\n",
        "    self.W_Q = nn.ModuleList() #The h different Query linear layers\n",
        "    self.W_K = nn.ModuleList() #The h different Key linear layers\n",
        "    self.W_V = nn.ModuleList() #The h different Value linear layers\n",
        "    self.Att = Attention(self.d_k, mask, context_len)\n",
        "    for i in range(h):\n",
        "      #Initialize all h linear layers for Q, K, V\n",
        "      self.W_Q.append(nn.Linear(d_model, self.d_k, bias=False))\n",
        "      self.W_K.append(nn.Linear(d_model, self.d_k, bias=False))\n",
        "      self.W_V.append(nn.Linear(d_model, self.d_k, bias=False)) #TODO: Instead of Linear layers, just create Matrices\n",
        "\n",
        "  def forward(self, Q, K, V):\n",
        "    #The inputs are Queries, Keys and Vectors which each have size seq x d_model\n",
        "    if (Q.shape[1] != self.d_model or K.shape[1] != self.d_model or V.shape[1] != self.d_model):\n",
        "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
        "    heads = []\n",
        "    for i in range(self.h):\n",
        "      queries = self.W_Q[i](Q)\n",
        "      keys = self.W_K[i](K)\n",
        "      values = self.W_V[i](V)\n",
        "      #At this point, queries keys & values have dimensions seq x d_k\n",
        "      heads.append(self.Att.forward(queries, keys, values))\n",
        "    out = torch.cat(heads, 1) #The output has the same dimension as all the inputs: seq x d_model\n",
        "    out = self.W_O(out)\n",
        "    return out\n",
        "\n",
        "#Define a Feed Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_hidden):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential(\n",
        "        nn.Linear(d_model, d_hidden, bias=True),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(d_hidden, d_model, bias=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.network(x)\n",
        "    return x\n",
        "\n",
        "#Define Layer Normalization:\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.epsilon = 1e-5\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "  def __call__(self, x):\n",
        "    mean = x.mean(1, keepdim=True) #Mean across the layer i.e. the column\n",
        "    variance = x.var(1, keepdim=True) #Mean across the layer i.e. column\n",
        "    norm = (x - mean) / torch.sqrt(variance + self.epsilon) #Normalize\n",
        "    out = self.gamma * norm + self.beta #Scale by gamma, add beta to achieve var= gamma, mean = beta\n",
        "    return out\n",
        "\n",
        "#Define batch normalization\n",
        "class BatchNorm(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model):\n",
        "    super().__init__()\n",
        "    self.epsilon = 1e-5\n",
        "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
        "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "  def __call__(self, x):\n",
        "    mean = x.mean(0, keepdim=True) #Mean across the batch i.e. row\n",
        "    variance = x.var(0, keepdim=True) #Mean across the batch i.e. row\n",
        "    norm = (x - mean) / torch.sqrt(variance + self.epsilon) #Normalize\n",
        "    out = self.gamma * norm + self.beta #Scale by gamma, add beta to achieve var= gamma, mean = beta\n",
        "    return out\n",
        "\n",
        "#Define an Block of Multi Head Self-Attention with a Residual Connection & Layer Normalization\n",
        "class NormalizedSelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, h, mask, context_len):\n",
        "    super().__init__()\n",
        "    self.MHA = MultiHeadAttention(d_model, h, mask, context_len)\n",
        "    self.norm = LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.MHA(x, x, x)\n",
        "    x = self.norm(x)\n",
        "    return x\n",
        "\n",
        "#Define a block of a Feed Forward Network with a Residual Connection & Layer Normalization\n",
        "class NormalizedFeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, d_model, d_hidden):\n",
        "    super().__init__()\n",
        "    self.FF = FeedForward(d_model, d_hidden)\n",
        "    self.norm = LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.FF(x)\n",
        "    x = self.norm(x)\n",
        "    return x\n",
        "\n",
        "#Define a stand-alone Encoder block with N-self attention and feed forward blocks (without embeddings or softmax)\n",
        "#This is identical to StandAloneDecoder except that masking is set to false, so attention is bi-directional\n",
        "class StandAloneEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self, N, d_model, h, d_hidden, context_len):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential()\n",
        "    for i in range(N):\n",
        "      self.network.append(NormalizedSelfAttention(d_model, h, False, context_len))\n",
        "      self.network.append(NormalizedFeedForward(d_model, d_hidden))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.network(x)\n",
        "    return out\n",
        "\n",
        "#Define a stand-alone Decoder block with N self-attention and feed forward blocks (without embeddings or softmax)\n",
        "#Note that the self attention is Masked because this is a Decoder\n",
        "class StandAloneDecoder(nn.Module):\n",
        "\n",
        "  def __init__(self, N, d_model, h, d_hidden, context_len):\n",
        "    super().__init__()\n",
        "    self.network = nn.Sequential()\n",
        "    for i in range(N):\n",
        "      self.network.append(NormalizedSelfAttention(d_model, h, True, context_len))\n",
        "      self.network.append(NormalizedFeedForward(d_model, d_hidden))\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.network(x)\n",
        "    return out\n",
        "\n",
        "#Define an entire Transformer with E encoder blocks and D decoder blocks (without embeddings or softmax)\n",
        "#The last encoder provides input to all D decoders\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self, E, D, d_model, h, d_hidden, context_len):\n",
        "    super().__init__()\n",
        "    self.E = E\n",
        "    self.D = D\n",
        "    self.encoders = nn.Sequential()\n",
        "    for e in range(E):\n",
        "      self.encoders.append(NormalizedSelfAttention(d_model, h, False, context_len))\n",
        "      self.encoders.append(NormalizedFeedForward(d_model, d_hidden))\n",
        "    self.decoders = nn.ModuleList()\n",
        "    for d in range(D):\n",
        "      self.decoders.append(NormalizedSelfAttention(d_model, h, True, context_len))\n",
        "      self.decoders.append(MultiHeadAttention(d_model, h, False, context_len))\n",
        "      self.decoders.append(LayerNorm(d_model))\n",
        "      self.decoders.append(NormalizedFeedForward(d_model, d_hidden))\n",
        "\n",
        "  def __forward__(self, inputs, outputs):\n",
        "    inputs = self.encoders(inputs)\n",
        "    for e in range(self.E):\n",
        "      outputs = self.decoders[3*e](outputs) #self attention\n",
        "      outputs = self.decoders[3*e+1](outputs, inputs, inputs) #cross attention\n",
        "      outputs = outputs + self.decoders[3*e+2](outputs) #Layer norm & residual connection\n",
        "      outputs = self.decoders[3*e+3](outputs) #Feed forward\n",
        "    return outputs\n",
        "#Test what happens when a cross attention head gets inputs of:\n",
        "#Queries: a x d_model\n",
        "#Keys & Values: b x d_model\n",
        "#Where a != b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Global hyperparam\n",
        "max_context = 8"
      ],
      "metadata": {
        "id": "uWRi_CNStwQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing NormalizedFeedForward\n",
        "d_model = 32\n",
        "d_hidd = 64\n",
        "seq = 3\n",
        "normFeed = NormalizedFeedForward(d_model, d_hidd)\n",
        "input = torch.ones(seq, d_model)\n",
        "output = normFeed(input)\n",
        "print(output)\n",
        "print(output[0,:].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwxGsN8KtNno",
        "outputId": "6b3789cf-d4dd-4fc1-9990-7b99c72100fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.9178e-01,  4.3697e-04, -8.7427e-01,  4.5068e-02,  3.5812e-01,\n",
            "          1.1397e+00,  1.0114e+00,  1.0266e+00,  2.5649e-01, -6.8877e-01,\n",
            "          1.0952e+00,  1.0583e+00,  1.9244e-01, -8.6496e-01,  5.6941e-01,\n",
            "          4.1225e-01, -2.7264e+00, -1.0703e+00,  7.7482e-01,  1.7110e+00,\n",
            "          9.4203e-01,  5.2723e-01, -1.1681e+00, -3.4162e-01, -1.4807e+00,\n",
            "         -8.5708e-01,  1.2498e+00, -1.3937e-02, -1.3199e-01, -1.5662e+00,\n",
            "          8.3566e-03, -8.8612e-01],\n",
            "        [ 2.9178e-01,  4.3697e-04, -8.7427e-01,  4.5068e-02,  3.5812e-01,\n",
            "          1.1397e+00,  1.0114e+00,  1.0266e+00,  2.5649e-01, -6.8877e-01,\n",
            "          1.0952e+00,  1.0583e+00,  1.9244e-01, -8.6496e-01,  5.6941e-01,\n",
            "          4.1225e-01, -2.7264e+00, -1.0703e+00,  7.7482e-01,  1.7110e+00,\n",
            "          9.4203e-01,  5.2723e-01, -1.1681e+00, -3.4162e-01, -1.4807e+00,\n",
            "         -8.5708e-01,  1.2498e+00, -1.3937e-02, -1.3199e-01, -1.5662e+00,\n",
            "          8.3566e-03, -8.8612e-01],\n",
            "        [ 2.9178e-01,  4.3697e-04, -8.7427e-01,  4.5068e-02,  3.5812e-01,\n",
            "          1.1397e+00,  1.0114e+00,  1.0266e+00,  2.5649e-01, -6.8877e-01,\n",
            "          1.0952e+00,  1.0583e+00,  1.9244e-01, -8.6496e-01,  5.6941e-01,\n",
            "          4.1225e-01, -2.7264e+00, -1.0703e+00,  7.7482e-01,  1.7110e+00,\n",
            "          9.4203e-01,  5.2723e-01, -1.1681e+00, -3.4162e-01, -1.4807e+00,\n",
            "         -8.5708e-01,  1.2498e+00, -1.3937e-02, -1.3199e-01, -1.5662e+00,\n",
            "          8.3566e-03, -8.8612e-01]], grad_fn=<AddBackward0>)\n",
            "tensor(0.9999, grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing NormalizedSelfAttention\n",
        "d_model = 32\n",
        "heads = 8\n",
        "seq = 3\n",
        "selfAtt = NormalizedSelfAttention(d_model, heads, True, max_context)\n",
        "input = torch.ones(seq, d_model)\n",
        "output = selfAtt(input)\n",
        "print(output)\n",
        "print(output[0,:].std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0sX2BGFsdst",
        "outputId": "2bf56e9a-fd11-4d01-e416-0ae85e81479a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.5378,  0.5970,  0.9667, -0.6975,  0.2002, -1.5745, -0.6936,  0.8485,\n",
            "         -0.3678,  1.9080, -0.2445, -1.1252, -1.3877,  0.0942, -0.6123, -0.1480,\n",
            "          0.5845, -2.1231,  0.1803,  1.1711, -0.3985, -0.6275, -0.5578,  0.6858,\n",
            "         -0.3138,  1.7304, -0.5328, -0.1623, -0.8022,  0.2883,  2.4591,  0.1171],\n",
            "        [ 0.5378,  0.5970,  0.9667, -0.6975,  0.2002, -1.5745, -0.6936,  0.8485,\n",
            "         -0.3678,  1.9080, -0.2445, -1.1252, -1.3877,  0.0942, -0.6123, -0.1480,\n",
            "          0.5845, -2.1231,  0.1803,  1.1711, -0.3985, -0.6275, -0.5578,  0.6858,\n",
            "         -0.3138,  1.7304, -0.5328, -0.1623, -0.8022,  0.2883,  2.4591,  0.1171],\n",
            "        [ 0.5378,  0.5970,  0.9667, -0.6975,  0.2002, -1.5745, -0.6936,  0.8485,\n",
            "         -0.3678,  1.9080, -0.2445, -1.1252, -1.3877,  0.0942, -0.6123, -0.1480,\n",
            "          0.5845, -2.1231,  0.1803,  1.1711, -0.3985, -0.6275, -0.5578,  0.6858,\n",
            "         -0.3138,  1.7304, -0.5328, -0.1623, -0.8022,  0.2883,  2.4591,  0.1171]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor(1.0000, grad_fn=<StdBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing layer norm\n",
        "randoms = torch.randn(2, 5)\n",
        "layernorm = LayerNorm(5)\n",
        "randoms = layernorm(randoms)\n",
        "\n",
        "print(\"Column mean and std dev\")\n",
        "print(randoms[:,0].mean())\n",
        "print(randoms[:,0].std())\n",
        "\n",
        "print(\"Row mean and std dev\")\n",
        "print(randoms[0,:].mean())\n",
        "print(randoms[0,:].std())\n",
        "\n",
        "print(\"We want the rows normalized\")\n",
        "print(randoms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl2QD0Muit60",
        "outputId": "5a8b9210-0693-4ce1-a115-d06b69a8ff27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column mean and std dev\n",
            "tensor(0.5284, grad_fn=<MeanBackward0>)\n",
            "tensor(1.3514, grad_fn=<StdBackward0>)\n",
            "Row mean and std dev\n",
            "tensor(-5.9605e-08, grad_fn=<MeanBackward0>)\n",
            "tensor(1.0000, grad_fn=<StdBackward0>)\n",
            "We want the rows normalized\n",
            "tensor([[ 1.4840,  0.5001, -0.7490, -0.9520, -0.2830],\n",
            "        [-0.4272,  0.1853, -0.5511, -0.8615,  1.6545]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of Feed Forward\n",
        "d_model = 64\n",
        "d_hidden = 256\n",
        "seq = 5\n",
        "ff = FeedForward(d_model, d_hidden)\n",
        "Input = torch.ones(seq, d_model)\n",
        "print(ff.forward(Input).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pj7CDHlR-22",
        "outputId": "e02bbe4e-331d-49d3-d7aa-8e1e4cab2d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of MultiHeadAttention\n",
        "#Input is sequence size by d_model\n",
        "#Q, K, V are all that size - in self attention Q=K=V=Input\n",
        "#Hyperparams:\n",
        "seq = 8 #Sequence size (i.e. number of characters). Must be less than blocksize.\n",
        "d_model = 16 #Size of the embedding vectors for each character\n",
        "heads = 8 #Number of heads in our multihead attention\n",
        "mult = MultiHeadAttention(d_model, heads, True, max_context)\n",
        "Input = torch.ones(seq, d_model)\n",
        "output = mult.forward(Input, Input, Input)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7vyhFYWCYUz",
        "outputId": "8a8696b6-7d64-4340-f928-27a1441d4266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 16])\n",
            "tensor([[ 0.2838, -0.4051,  0.2764,  0.5660,  0.2345, -0.0719,  0.1130, -0.2280,\n",
            "         -0.0888,  0.1520, -0.2332, -0.1001,  0.5237, -0.2018, -0.0421, -0.2075],\n",
            "        [ 0.2838, -0.4051,  0.2764,  0.5660,  0.2345, -0.0719,  0.1130, -0.2280,\n",
            "         -0.0888,  0.1520, -0.2332, -0.1001,  0.5237, -0.2018, -0.0421, -0.2075],\n",
            "        [ 0.2838, -0.4051,  0.2764,  0.5660,  0.2345, -0.0719,  0.1130, -0.2280,\n",
            "         -0.0888,  0.1520, -0.2332, -0.1001,  0.5237, -0.2018, -0.0421, -0.2075],\n",
            "        [ 0.2838, -0.4051,  0.2764,  0.5660,  0.2345, -0.0719,  0.1130, -0.2280,\n",
            "         -0.0888,  0.1520, -0.2332, -0.1001,  0.5237, -0.2018, -0.0421, -0.2075],\n",
            "        [ 0.2838, -0.4051,  0.2764,  0.5660,  0.2345, -0.0719,  0.1130, -0.2280,\n",
            "         -0.0888,  0.1520, -0.2332, -0.1001,  0.5237, -0.2018, -0.0421, -0.2075],\n",
            "        [ 0.2838, -0.4051,  0.2764,  0.5660,  0.2345, -0.0719,  0.1130, -0.2280,\n",
            "         -0.0888,  0.1520, -0.2332, -0.1001,  0.5237, -0.2018, -0.0421, -0.2075],\n",
            "        [ 0.2838, -0.4051,  0.2764,  0.5660,  0.2345, -0.0719,  0.1130, -0.2280,\n",
            "         -0.0888,  0.1520, -0.2332, -0.1001,  0.5237, -0.2018, -0.0421, -0.2075],\n",
            "        [ 0.2838, -0.4051,  0.2764,  0.5660,  0.2345, -0.0719,  0.1130, -0.2280,\n",
            "         -0.0888,  0.1520, -0.2332, -0.1001,  0.5237, -0.2018, -0.0421, -0.2075]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For testing & validation of Attention\n",
        "#Q, K, V all have size seq x d_k\n",
        "#Hyperparams:\n",
        "seq = 4 #Sequence length (i.e. number of characters)\n",
        "d_k = 5 #dimension of embedding vectors in this head\n",
        "att = Attention(d_k, True, max_context)\n",
        "Input = torch.ones(seq, d_k)\n",
        "print(att.forward(Input, Input, Input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOooydTg0Xw9",
        "outputId": "a540e32c-a59e-48c4-8218-901ebce486a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    }
  ]
}