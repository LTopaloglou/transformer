{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d62e5b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-11T16:48:37.975466Z",
     "iopub.status.busy": "2024-05-11T16:48:37.975132Z",
     "iopub.status.idle": "2024-05-11T16:48:38.696578Z",
     "shell.execute_reply": "2024-05-11T16:48:38.695487Z"
    },
    "papermill": {
     "duration": 0.729949,
     "end_time": "2024-05-11T16:48:38.698550",
     "exception": false,
     "start_time": "2024-05-11T16:48:37.968601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/digit-recognizer/sample_submission.csv\n",
      "/kaggle/input/digit-recognizer/train.csv\n",
      "/kaggle/input/digit-recognizer/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef79471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T16:48:38.709604Z",
     "iopub.status.busy": "2024-05-11T16:48:38.709213Z",
     "iopub.status.idle": "2024-05-11T16:48:42.345074Z",
     "shell.execute_reply": "2024-05-11T16:48:42.344281Z"
    },
    "papermill": {
     "duration": 3.64392,
     "end_time": "2024-05-11T16:48:42.347317",
     "exception": false,
     "start_time": "2024-05-11T16:48:38.703397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Library\n",
    "from torch.torch_version import Version\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#I am using variable names defined in \"Attention Is All You Need\" for simplicity.\n",
    "\n",
    "#Define a single Scaled Dot-Product Attention head. I am simplifying it by enforcing d_k = d_v\n",
    "class Attention(nn.Module):\n",
    "\n",
    "  def __init__(self, d_k, mask, context_len):\n",
    "    super().__init__()\n",
    "    self.mask = mask\n",
    "    self.d_k = d_k\n",
    "    self.register_buffer('lowerTriangle', torch.tril(torch.ones(context_len, context_len)))\n",
    "\n",
    "  def forward(self, Q, K, V):\n",
    "    #Takes in vectors of Queries, Keys & Values. Queries, Keys & Values have dimension seq x d_k\n",
    "    if (Q.shape[1] != self.d_k or K.shape[1] != self.d_k or V.shape[1] != self.d_k):\n",
    "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
    "    seq = Q.shape[0]\n",
    "    #First take the dot product of Queries & Keys. Weight has dimensions seq x seq\n",
    "    weight = Q @ K.transpose(0, 1)\n",
    "    #Now scale by 1/sqrt(d_k)\n",
    "    weight = weight  * (self.d_k**-0.5)\n",
    "    #Mask everything not in the lower triangular of weight\n",
    "    if (self.mask):\n",
    "      weight = weight.masked_fill(self.lowerTriangle[:seq, :seq]== 0, float('-inf'))\n",
    "    #Now apply softmax in the dimension of the rows\n",
    "    weight = weight.softmax(1)\n",
    "    #Finally, multiply values by weights to get the outputs\n",
    "    out = weight @ V # [seq, seq] x [seq, d_k] = [seq, d_k]\n",
    "    return out\n",
    "\n",
    "#Define a Multi-Head Attention layer. As in the paper, I am setting d_k = d_model/h\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model, h, mask, context_len):\n",
    "    super().__init__()\n",
    "    self.mask = mask\n",
    "    self.d_model = d_model\n",
    "    self.h = h\n",
    "    self.d_k = int(d_model/h)\n",
    "    if (self.d_k != d_model/h):\n",
    "      raise Exception('Invalid Dimensions Provided') #Ensure valid dimensions\n",
    "    self.W_O = nn.Linear(h*self.d_k, d_model, bias=False) #Output linear layer\n",
    "    self.W_Q = nn.ModuleList() #The h different Query linear layers\n",
    "    self.W_K = nn.ModuleList() #The h different Key linear layers\n",
    "    self.W_V = nn.ModuleList() #The h different Value linear layers\n",
    "    self.Att = Attention(self.d_k, mask, context_len)\n",
    "    for i in range(h):\n",
    "      #Initialize all h linear layers for Q, K, V\n",
    "      self.W_Q.append(nn.Linear(d_model, self.d_k, bias=False))\n",
    "      self.W_K.append(nn.Linear(d_model, self.d_k, bias=False))\n",
    "      self.W_V.append(nn.Linear(d_model, self.d_k, bias=False)) #TODO: Instead of Linear layers, just create Matrices\n",
    "\n",
    "  def forward(self, Q, K, V):\n",
    "    #The inputs are Queries, Keys and Vectors which each have size seq x d_model\n",
    "    if (Q.shape[1] != self.d_model or K.shape[1] != self.d_model or V.shape[1] != self.d_model):\n",
    "      raise Exception('Invalid Query, Key or Value Dimensions')\n",
    "    heads = []\n",
    "    for i in range(self.h):\n",
    "      queries = self.W_Q[i](Q)\n",
    "      keys = self.W_K[i](K)\n",
    "      values = self.W_V[i](V)\n",
    "      #At this point, queries keys & values have dimensions seq x d_k\n",
    "      heads.append(self.Att.forward(queries, keys, values))\n",
    "    out = torch.cat(heads, 1) #The output has the same dimension as all the inputs: seq x d_model\n",
    "    out = self.W_O(out)\n",
    "    return out\n",
    "\n",
    "#Define a Feed Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model, d_hidden):\n",
    "    super().__init__()\n",
    "    self.network = nn.Sequential(\n",
    "        nn.Linear(d_model, d_hidden, bias=True),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(d_hidden, d_model, bias=True)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.network(x)\n",
    "    return x\n",
    "\n",
    "#Define Layer Normalization:\n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model):\n",
    "    super().__init__()\n",
    "    self.epsilon = 1e-5\n",
    "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    mean = x.mean(1, keepdim=True) #Mean across the layer i.e. the column\n",
    "    variance = x.var(1, keepdim=True) #Mean across the layer i.e. column\n",
    "    norm = (x - mean) / torch.sqrt(variance + self.epsilon) #Normalize\n",
    "    out = self.gamma * norm + self.beta #Scale by gamma, add beta to achieve var= gamma, mean = beta\n",
    "    return out\n",
    "\n",
    "#Define batch normalization\n",
    "class BatchNorm(nn.Module):\n",
    "    \n",
    "  def __init__(self, d_model):\n",
    "    super().__init__()\n",
    "    self.epsilon = 1e-5\n",
    "    self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "    self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    mean = x.mean(0, keepdim=True) #Mean across the batch i.e. row\n",
    "    variance = x.var(0, keepdim=True) #Mean across the batch i.e. row\n",
    "    norm = (x - mean) / torch.sqrt(variance + self.epsilon) #Normalize\n",
    "    out = self.gamma * norm + self.beta #Scale by gamma, add beta to achieve var= gamma, mean = beta\n",
    "    return out\n",
    "\n",
    "#Define an Block of Multi Head Self-Attention with a Residual Connection & Layer Normalization\n",
    "class NormalizedSelfAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model, h, mask, context_len):\n",
    "    super().__init__()\n",
    "    self.MHA = MultiHeadAttention(d_model, h, mask, context_len)\n",
    "    self.norm = BatchNorm(d_model)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.MHA(x, x, x)\n",
    "    x = self.norm(x)\n",
    "    return x\n",
    "\n",
    "#Define a block of a Feed Forward Network with a Residual Connection & Layer Normalization\n",
    "class NormalizedFeedForward(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model, d_hidden):\n",
    "    super().__init__()\n",
    "    self.FF = FeedForward(d_model, d_hidden)\n",
    "    self.norm = BatchNorm(d_model)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.FF(x)\n",
    "    x = self.norm(x)\n",
    "    return x\n",
    "\n",
    "#Define a stand-alone Encoder block with N-self attention and feed forward blocks (without embeddings or softmax)\n",
    "#This is identical to StandAloneDecoder except that masking is set to false, so attention is bi-directional\n",
    "class StandAloneEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self, N, d_model, h, d_hidden, context_len):\n",
    "    super().__init__()\n",
    "    self.network = nn.Sequential()\n",
    "    for i in range(N):\n",
    "      self.network.append(NormalizedSelfAttention(d_model, h, False, context_len))\n",
    "      self.network.append(NormalizedFeedForward(d_model, d_hidden))\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.network(x)\n",
    "    return out\n",
    "\n",
    "#Define a stand-alone Decoder block with N self-attention and feed forward blocks (without embeddings or softmax)\n",
    "#Note that the self attention is Masked because this is a Decoder\n",
    "class StandAloneDecoder(nn.Module):\n",
    "\n",
    "  def __init__(self, N, d_model, h, d_hidden, context_len):\n",
    "    super().__init__()\n",
    "    self.network = nn.Sequential()\n",
    "    for i in range(N):\n",
    "      self.network.append(NormalizedSelfAttention(d_model, h, True, context_len))\n",
    "      self.network.append(NormalizedFeedForward(d_model, d_hidden))\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.network(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef256934",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T16:48:42.358602Z",
     "iopub.status.busy": "2024-05-11T16:48:42.358163Z",
     "iopub.status.idle": "2024-05-11T16:48:42.411159Z",
     "shell.execute_reply": "2024-05-11T16:48:42.410124Z"
    },
    "papermill": {
     "duration": 0.060941,
     "end_time": "2024-05-11T16:48:42.413198",
     "exception": false,
     "start_time": "2024-05-11T16:48:42.352257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set up pytorch to run on GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    processor = 'cuda'\n",
    "else:\n",
    "    processor = 'cpu'\n",
    "device = torch.device(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec894092",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T16:48:42.424773Z",
     "iopub.status.busy": "2024-05-11T16:48:42.423953Z",
     "iopub.status.idle": "2024-05-11T16:48:46.783818Z",
     "shell.execute_reply": "2024-05-11T16:48:46.782738Z"
    },
    "papermill": {
     "duration": 4.368112,
     "end_time": "2024-05-11T16:48:46.786271",
     "exception": false,
     "start_time": "2024-05-11T16:48:42.418159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([42000])\n",
      "torch.Size([42000, 784])\n",
      "torch.float32\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627, 0.4627, 0.7373, 0.9961,\n",
      "        0.9961, 1.0000, 0.9961, 0.5961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.2784, 0.9176, 0.9255, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9843, 0.6000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.3216, 0.8824, 0.9922, 0.9922, 0.9922,\n",
      "        0.8392, 0.6941, 0.8000, 0.9922, 0.9922, 0.9922, 0.9882, 0.4314, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7882, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.4824, 0.0000, 0.0863, 0.2392, 0.6745, 0.9922, 0.9922, 0.8745,\n",
      "        0.0392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9176, 0.9922, 0.9922,\n",
      "        0.9922, 0.9843, 0.4196, 0.0000, 0.0000, 0.0000, 0.1922, 0.9922, 0.9922,\n",
      "        0.9922, 0.1255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5725, 0.9843,\n",
      "        0.9922, 0.9569, 0.8863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0784, 0.7569,\n",
      "        0.9922, 0.9922, 0.6078, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.4078, 0.4824, 0.1569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.1098, 0.9922, 0.9922, 0.9922, 0.1333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.1098, 0.9922, 0.9922, 0.9922, 0.1333, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.3529, 0.5843, 0.8902,\n",
      "        0.4549, 0.3529, 0.7137, 0.9922, 0.9922, 0.9922, 0.1333, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0392, 0.6000, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8824, 0.0980, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1451, 0.5922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.1961, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1961, 0.9098, 0.9922, 0.9922,\n",
      "        0.9922, 0.8863, 0.8863, 0.9725, 0.9922, 0.9922, 0.9922, 0.9922, 0.1098,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3137, 0.9922, 0.9922,\n",
      "        0.9608, 0.4275, 0.0118, 0.0118, 0.8157, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.8196, 0.0902, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8353, 0.9922,\n",
      "        0.9922, 0.9373, 0.0000, 0.2118, 0.6627, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "        0.9922, 0.9922, 0.6667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3255,\n",
      "        0.9922, 0.9922, 0.9882, 0.9451, 0.9647, 0.9922, 0.9922, 0.9922, 0.9490,\n",
      "        0.8196, 0.9922, 0.9922, 0.8000, 0.0863, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.2039, 0.7373, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.8275,\n",
      "        0.1176, 0.3843, 0.8275, 0.9922, 0.9922, 0.8902, 0.4314, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0471, 0.5765, 0.8941, 0.9922, 0.9922, 0.9922, 0.7961,\n",
      "        0.3176, 0.0000, 0.0000, 0.1059, 0.9529, 0.9922, 0.9922, 0.9569, 0.4275,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.2941, 0.2941, 0.2941,\n",
      "        0.0549, 0.0000, 0.0000, 0.0000, 0.0000, 0.4980, 0.9725, 0.9922, 0.9922,\n",
      "        0.9098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5255, 0.9922,\n",
      "        0.9922, 0.9098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1490,\n",
      "        0.8784, 0.9922, 0.6196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Load data, split into 90% train, 10% test\n",
    "test = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n",
    "test = test.sample(frac=1).reset_index(drop=True)\n",
    "n1 = int(len(test) * 0.9)\n",
    "\n",
    "#parse dataset as tensors\n",
    "label = torch.tensor(test.loc[:,'label'].to_numpy(), dtype=torch.int64)\n",
    "print(label.size())\n",
    "digits = torch.tensor(test.iloc[:, 1:].to_numpy(), dtype=torch.float32) / 255\n",
    "print(digits.size())\n",
    "print(digits.dtype)\n",
    "\n",
    "#Training, dev & test splits\n",
    "Xtr = digits\n",
    "Ytr = label\n",
    "Xtest = digits[n1:]\n",
    "Ytest = label[n1:]\n",
    "\n",
    "#Move all to gpu if possible\n",
    "Xtr = Xtr.to(device)\n",
    "Ytr = Ytr.to(device)\n",
    "Xtest = Xtest.to(device)\n",
    "Ytest = Ytest.to(device)\n",
    "\n",
    "#Print a single image embedding\n",
    "print(Xtr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c766f160",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T16:48:46.797454Z",
     "iopub.status.busy": "2024-05-11T16:48:46.797175Z",
     "iopub.status.idle": "2024-05-11T16:48:47.191915Z",
     "shell.execute_reply": "2024-05-11T16:48:47.190876Z"
    },
    "papermill": {
     "duration": 0.402615,
     "end_time": "2024-05-11T16:48:47.194025",
     "exception": false,
     "start_time": "2024-05-11T16:48:46.791410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "tensor(2, device='cuda:0')\n",
      "Loss\n",
      "tensor(2.1688, device='cuda:0', grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "#Choose hyperparameters\n",
    "num_layers = 8 #Number of encoder layers\n",
    "model_dim = 4 #We will project every (scalar) greyscale pixture into this many dimensions, and use that number in our transformer\n",
    "parallel_heads = 2 #Number of parallel attention heads\n",
    "hidden_dim = 784 #Output dimension of internal feed forward layers\n",
    "max_context = 784 #Max length of an input sequence (there are 784 pixels in each image)\n",
    "\n",
    "#Create model\n",
    "class VisionEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, model_dim, parallel_heads, hidden_dim, max_context):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(1, model_dim) #Learned projection from 1-d greyscale pixels to 4-d representation\n",
    "        self.encoder = StandAloneEncoder(num_layers, model_dim, parallel_heads, hidden_dim, max_context)\n",
    "        self.classifier1 = nn.Linear(max_context*model_dim, 10)\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        x = self.projection(x)\n",
    "        x = self.encoder(x).view(1, max_context*model_dim)\n",
    "        x = self.classifier1(x)\n",
    "        x = x.view(10)\n",
    "        test_probs = F.softmax(x, dim=0)\n",
    "        prediction = torch.argmax(test_probs, dim=0)\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            onehot = F.one_hot(target, num_classes=10).type(torch.float32).view(10).to(device)\n",
    "            loss = F.cross_entropy(test_probs, onehot)\n",
    "            \n",
    "        return prediction, loss\n",
    "\n",
    "model = VisionEncoder(num_layers, model_dim, parallel_heads, hidden_dim, max_context).to(device)\n",
    "                      \n",
    "#Sample forward pass\n",
    "#print(\"Input shape:\")\n",
    "test_input = Xtr[0].view(784, 1)\n",
    "#print(test_input.shape)\n",
    "#print(\"Input:\")\n",
    "#print(test_input)\n",
    "output, loss = model(test_input, Ytr[0])\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(\"Loss\")\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da100c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T16:48:47.205184Z",
     "iopub.status.busy": "2024-05-11T16:48:47.204910Z",
     "iopub.status.idle": "2024-05-11T16:48:47.214764Z",
     "shell.execute_reply": "2024-05-11T16:48:47.213791Z"
    },
    "papermill": {
     "duration": 0.017615,
     "end_time": "2024-05-11T16:48:47.216630",
     "exception": false,
     "start_time": "2024-05-11T16:48:47.199015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784, 1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "def get_example():\n",
    "    #Choose a random image from the dataset.\n",
    "    image_index = torch.randint(Xtr.shape[0], (1,))\n",
    "    x = Xtr[image_index].view(784, 1)\n",
    "    y = Ytr[image_index]\n",
    "    return x,y\n",
    "\n",
    "x_train, y_train = get_example()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_example()\n",
    "        prediction, loss = model(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_loss():\n",
    "    model.eval()\n",
    "    losses = torch.zeros(Xtest.shape[0])\n",
    "    for x, y, iteration in zip(Xtest, Ytest, range(Xtest.shape[0])):\n",
    "        x = x.view(784, 1)\n",
    "        prediction, loss = model(x, y)\n",
    "        losses[iteration] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d081f3ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T16:48:47.227409Z",
     "iopub.status.busy": "2024-05-11T16:48:47.227154Z",
     "iopub.status.idle": "2024-05-11T16:48:47.232880Z",
     "shell.execute_reply": "2024-05-11T16:48:47.232057Z"
    },
    "papermill": {
     "duration": 0.013311,
     "end_time": "2024-05-11T16:48:47.234846",
     "exception": false,
     "start_time": "2024-05-11T16:48:47.221535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(training_iters, eval_interval, eval_iters, optimizer):\n",
    "    for iter in range(training_iters):\n",
    "        #Evaluate training loss\n",
    "        if iter % eval_interval == 0 or iter == training_iters - 1:\n",
    "            losses = estimate_loss(eval_iters)\n",
    "            print(f\"step {iter}: train loss {losses.item():.4f}\")\n",
    "\n",
    "        #Get a training example\n",
    "        x,y = get_example()\n",
    "\n",
    "        #evaluate loss\n",
    "        prediction, loss = model(x, y)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "549b2f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T16:48:47.245491Z",
     "iopub.status.busy": "2024-05-11T16:48:47.245252Z",
     "iopub.status.idle": "2024-05-11T17:53:02.874719Z",
     "shell.execute_reply": "2024-05-11T17:53:02.873673Z"
    },
    "papermill": {
     "duration": 3855.637265,
     "end_time": "2024-05-11T17:53:02.876944",
     "exception": false,
     "start_time": "2024-05-11T16:48:47.239679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------New Epoch--------\n",
      "step 0: train loss 2.3174\n",
      "step 2100: train loss 1.7100\n",
      "step 4200: train loss 1.7903\n",
      "step 6300: train loss 1.6586\n",
      "step 8400: train loss 1.5966\n",
      "step 10500: train loss 1.6615\n",
      "step 12600: train loss 1.5564\n",
      "step 14700: train loss 1.6014\n",
      "step 16800: train loss 1.6167\n",
      "step 18900: train loss 1.5696\n",
      "step 21000: train loss 1.5901\n",
      "step 23100: train loss 1.5808\n",
      "step 25200: train loss 1.5594\n",
      "step 27300: train loss 1.5757\n",
      "step 29400: train loss 1.5747\n",
      "step 31500: train loss 1.5607\n",
      "step 33600: train loss 1.5776\n",
      "step 35700: train loss 1.6637\n",
      "step 37800: train loss 1.5021\n",
      "step 39900: train loss 1.6076\n",
      "step 41999: train loss 1.5435\n",
      "Validation loss:  tensor(1.5735)\n",
      "-------New Epoch--------\n",
      "step 0: train loss 1.5232\n",
      "step 2100: train loss 1.5551\n",
      "step 4200: train loss 1.6628\n",
      "step 6300: train loss 1.5789\n",
      "step 8400: train loss 1.6017\n",
      "step 10500: train loss 1.5415\n",
      "step 12600: train loss 1.5918\n",
      "step 14700: train loss 1.6127\n",
      "step 16800: train loss 1.5906\n",
      "step 18900: train loss 1.5225\n",
      "step 21000: train loss 1.5868\n",
      "step 23100: train loss 1.5970\n",
      "step 25200: train loss 1.5521\n",
      "step 27300: train loss 1.5087\n",
      "step 29400: train loss 1.5772\n",
      "step 31500: train loss 1.5657\n",
      "step 33600: train loss 1.5307\n",
      "step 35700: train loss 1.5804\n",
      "step 37800: train loss 1.5795\n",
      "step 39900: train loss 1.5423\n",
      "step 41999: train loss 1.5218\n",
      "Validation loss:  tensor(1.5582)\n",
      "-------New Epoch--------\n",
      "step 0: train loss 1.6280\n",
      "step 2100: train loss 1.5249\n",
      "step 4200: train loss 1.5688\n",
      "step 6300: train loss 1.5808\n",
      "step 8400: train loss 1.6201\n",
      "step 10500: train loss 1.6019\n",
      "step 12600: train loss 1.5893\n",
      "step 14700: train loss 1.5043\n",
      "step 16800: train loss 1.5447\n",
      "step 18900: train loss 1.5444\n",
      "step 21000: train loss 1.5897\n",
      "step 23100: train loss 1.5843\n",
      "step 25200: train loss 1.5678\n",
      "step 27300: train loss 1.5645\n",
      "step 29400: train loss 1.5248\n",
      "step 31500: train loss 1.5422\n",
      "step 33600: train loss 1.5420\n",
      "step 35700: train loss 1.6233\n",
      "step 37800: train loss 1.5927\n",
      "step 39900: train loss 1.5967\n",
      "step 41999: train loss 1.5551\n",
      "Validation loss:  tensor(1.5601)\n",
      "-------New Epoch--------\n",
      "step 0: train loss 1.5560\n",
      "step 2100: train loss 1.5711\n",
      "step 4200: train loss 1.5866\n",
      "step 6300: train loss 1.5301\n",
      "step 8400: train loss 1.5763\n",
      "step 10500: train loss 1.5213\n",
      "step 12600: train loss 1.5786\n",
      "step 14700: train loss 1.5271\n",
      "step 16800: train loss 1.4851\n",
      "step 18900: train loss 1.5533\n",
      "step 21000: train loss 1.5781\n",
      "step 23100: train loss 1.5366\n",
      "step 25200: train loss 1.4968\n",
      "step 27300: train loss 1.5298\n",
      "step 29400: train loss 1.5500\n",
      "step 31500: train loss 1.5167\n",
      "step 33600: train loss 1.5418\n",
      "step 35700: train loss 1.5464\n",
      "step 37800: train loss 1.5556\n",
      "step 39900: train loss 1.5261\n",
      "step 41999: train loss 1.4927\n",
      "Validation loss:  tensor(1.5369)\n"
     ]
    }
   ],
   "source": [
    "#Setup optimizer, train\n",
    "epoch_size = Xtr.shape[0]\n",
    "training_program = [(1, 1e-3), (1, 1e-3), (1, 1e-3), (1, 1e-4)] #Each tuple is (# epochs, learning rate)\n",
    "evaluation_interval = round(epoch_size / 20)\n",
    "evaluation_iterations = round(epoch_size / 500)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "for epochs, learning_rate in training_program:\n",
    "    print(\"-------New Epoch--------\")\n",
    "    optimizer.param_groups[0]['lr'] = learning_rate\n",
    "    train(round(epoch_size * epochs), evaluation_interval, evaluation_iterations, optimizer)\n",
    "    print(\"Validation loss: \", validation_loss())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfaca56",
   "metadata": {
    "papermill": {
     "duration": 0.011614,
     "end_time": "2024-05-11T17:53:02.900973",
     "exception": false,
     "start_time": "2024-05-11T17:53:02.889359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Val loss: 1.5400, N=4, 1 layer classifier    \n",
    "Val loss: 1.5371, N=8, 1 layer classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c8371d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T17:53:02.925793Z",
     "iopub.status.busy": "2024-05-11T17:53:02.925364Z",
     "iopub.status.idle": "2024-05-11T17:53:34.812764Z",
     "shell.execute_reply": "2024-05-11T17:53:34.811734Z"
    },
    "papermill": {
     "duration": 31.914962,
     "end_time": "2024-05-11T17:53:34.827692",
     "exception": false,
     "start_time": "2024-05-11T17:53:02.912730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate:  0.925952380952381\n",
      "Successes:  3889\n",
      "Failures:  311\n"
     ]
    }
   ],
   "source": [
    "#Get percentage success rate on test data\n",
    "success = 0\n",
    "fail = 0\n",
    "model.eval()\n",
    "for x, y in zip(Xtest, Ytest):\n",
    "    x = x.view(784, 1)\n",
    "    prediction, loss = model(x, y)\n",
    "    #print(\"Prediction: \", prediction)\n",
    "    #print(\"Expected: \", y.item())\n",
    "    if prediction == y.item():\n",
    "        success += 1\n",
    "    else:\n",
    "        fail += 1\n",
    "model.train()\n",
    "success_rate = success / Xtest.shape[0]\n",
    "print(\"Success rate: \", success_rate)\n",
    "print(\"Successes: \", success)\n",
    "print(\"Failures: \", fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abccbce7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T17:53:34.854060Z",
     "iopub.status.busy": "2024-05-11T17:53:34.853444Z",
     "iopub.status.idle": "2024-05-11T17:57:05.764531Z",
     "shell.execute_reply": "2024-05-11T17:57:05.763712Z"
    },
    "papermill": {
     "duration": 210.926633,
     "end_time": "2024-05-11T17:57:05.767098",
     "exception": false,
     "start_time": "2024-05-11T17:53:34.840465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images:  28000\n"
     ]
    }
   ],
   "source": [
    "#Load test data, run inference and save the output for submission.\n",
    "test_data = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\n",
    "print(\"Number of test images: \", test_data.shape[0])\n",
    "\n",
    "#parse dat as tensor\n",
    "test_digits = torch.tensor(test_data.to_numpy(), dtype=torch.float32) / 255\n",
    "\n",
    "#Move all to gpu if possible\n",
    "test_digits = test_digits.to(device)\n",
    "\n",
    "\n",
    "#Run prediction\n",
    "results = []\n",
    "for x in test_digits:\n",
    "    x = x.view(784, 1)\n",
    "    prediction, _ = model(x, None)\n",
    "    results.append(prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c36c0a00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T17:57:05.792848Z",
     "iopub.status.busy": "2024-05-11T17:57:05.792514Z",
     "iopub.status.idle": "2024-05-11T17:57:05.821296Z",
     "shell.execute_reply": "2024-05-11T17:57:05.820400Z"
    },
    "papermill": {
     "duration": 0.044278,
     "end_time": "2024-05-11T17:57:05.823765",
     "exception": false,
     "start_time": "2024-05-11T17:57:05.779487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results:  28000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27995</th>\n",
       "      <td>27996</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>27997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27997</th>\n",
       "      <td>27998</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>27999</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27999</th>\n",
       "      <td>28000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ImageId  Label\n",
       "0            1      2\n",
       "1            2      0\n",
       "2            3      9\n",
       "3            4      9\n",
       "4            5      2\n",
       "...        ...    ...\n",
       "27995    27996      9\n",
       "27996    27997      7\n",
       "27997    27998      3\n",
       "27998    27999      9\n",
       "27999    28000      2\n",
       "\n",
       "[28000 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save results in dataframe\n",
    "submission = pd.DataFrame({'ImageId': range(1, len(results) + 1), 'Label': results})\n",
    "print(\"Number of results: \", len(results))\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e1c2fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-11T17:57:05.853801Z",
     "iopub.status.busy": "2024-05-11T17:57:05.853413Z",
     "iopub.status.idle": "2024-05-11T17:57:05.900266Z",
     "shell.execute_reply": "2024-05-11T17:57:05.899259Z"
    },
    "papermill": {
     "duration": 0.066028,
     "end_time": "2024-05-11T17:57:05.902848",
     "exception": false,
     "start_time": "2024-05-11T17:57:05.836820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4112.938221,
   "end_time": "2024-05-11T17:57:08.112372",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-11T16:48:35.174151",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
